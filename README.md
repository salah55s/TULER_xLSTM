# TULER_xLSTM

𝐓𝐔𝐋𝐄𝐑𝐦𝐋𝐒𝐓𝐌 𝟏,𝟐 🚀🚀

🧠𝐁𝐫𝐚𝐢𝐧-𝐈𝐧𝐬𝐩𝐢𝐫𝐞𝐝 𝐓𝐰𝐞𝐚𝐤 𝐁𝐨𝐨𝐬𝐭𝐬 𝐦𝐋𝐒𝐓𝐌 𝐏𝐞𝐫𝐟𝐨𝐫𝐦𝐚𝐧𝐜𝐞!🧠

Inspired by the way our brains work, I made some changes to Andrew Gao's awesome mLSTM implementation (best one out there! -> [https://lnkd.in/dqYM92-T]). 

𝐓𝐔𝐋𝐄𝐑𝐦𝐋𝐒𝐓𝐌 consumes the same computing resources as mLSTM. While it may take a little bit longer to converge during training, it demonstrates significantly better generalization and a greater potential to capture complex patterns, just like the way our brains do!

It's still early days, but I'm excited about the potential of this brain-inspired xLSTM for language modeling. Let's see how far we can push it!

For those new to xLSTM or mLSTM, check out the original paper: 👉👉https://lnkd.in/d8X88sfv

![alt text](https://github.com/salah55s/TULER_xLSTM//blob/main/Artboard2@4x.png?raw=true)
![alt text](https://github.com/salah55s/TULER_xLSTM//blob/main/Artboard5@4x.png?raw=true)
