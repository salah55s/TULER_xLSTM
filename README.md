# TULER_xLSTM

ğ“ğ”ğ‹ğ„ğ‘ğ¦ğ‹ğ’ğ“ğŒ ğŸ,ğŸ ğŸš€ğŸš€

ğŸ§ ğğ«ğšğ¢ğ§-ğˆğ§ğ¬ğ©ğ¢ğ«ğğ ğ“ğ°ğğšğ¤ ğğ¨ğ¨ğ¬ğ­ğ¬ ğ¦ğ‹ğ’ğ“ğŒ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ!ğŸ§ 

Inspired by the way our brains work, I made some changes to Andrew Gao's awesome mLSTM implementation (best one out there! -> [https://lnkd.in/dqYM92-T]). 

ğ“ğ”ğ‹ğ„ğ‘ğ¦ğ‹ğ’ğ“ğŒ consumes the same computing resources as mLSTM. While it may take a little bit longer to converge during training, it demonstrates significantly better generalization and a greater potential to capture complex patterns, just like the way our brains do!

It's still early days, but I'm excited about the potential of this brain-inspired xLSTM for language modeling. Let's see how far we can push it!

For those new to xLSTM or mLSTM, check out the original paper: ğŸ‘‰ğŸ‘‰https://lnkd.in/d8X88sfv

![alt text](https://github.com/salah55s/TULER_xLSTM//blob/main/Artboard2@4x.png?raw=true)
![alt text](https://github.com/salah55s/TULER_xLSTM//blob/main/Artboard5@4x.png?raw=true)
